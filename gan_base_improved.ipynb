{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8471ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 11:47:20.585564: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 11:47:20.587415: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-16 11:47:20.594887: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-16 11:47:20.614582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747388840.647428    8686 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747388840.656713    8686 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747388840.681167    8686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747388840.681204    8686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747388840.681209    8686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747388840.681213    8686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-16 11:47:20.690840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68947e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 11:47:26.476706: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "(train_images, _), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "train_images = (train_images.astype(\"float32\") - 127.5) / 127.5\n",
    "train_images = tf.expand_dims(train_images, -1)\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train_images).shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd88e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, losses, metrics\n",
    "\n",
    "\n",
    "# === 2. DISCRIMINATOR ===\n",
    "def build_discriminator():\n",
    "    inp = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(64, 5, strides=2, padding='same')(inp)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 5, strides=2, padding='same')(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1)(x)  # No sigmoid, logits output\n",
    "    return models.Model(inp, x, name=\"Discriminator\")\n",
    "\n",
    "\n",
    "# === 3. GENERATOR ===\n",
    "def build_generator(latent_dim=100):\n",
    "    inp = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(7 * 7 * 128, use_bias=False)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((7, 7, 128))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    '''\n",
    "    Dropout randomly deactivates neurons during training, \n",
    "    helping the generator avoid overfitting to early discriminator \n",
    "    feedback and encouraging it to explore more varied outputs — \n",
    "    improving sample diversity and training robustness.\n",
    "    '''\n",
    "    x = layers.Dropout(0.3)(x)  # Improvement\n",
    "\n",
    "    x = layers.Conv2DTranspose(1, 4, strides=2, padding='same', activation='tanh')(x)\n",
    "    return models.Model(inp, x, name=\"Generator\")\n",
    "\n",
    "# === 4. DCGAN TRAINING ===\n",
    "class DCGAN(models.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim, label_flipping_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_flipping_rate = label_flipping_rate\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super().compile()\n",
    "        self.loss_fn = losses.BinaryCrossentropy()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent = tf.random.normal((batch_size, self.latent_dim))\n",
    "\n",
    "        # === DISCRIMINATOR TRAINING ===\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake_images = self.generator(random_latent, training=True)\n",
    "            '''\n",
    "            Adding noise to real images prevents the discriminator\n",
    "            from memorizing the training data or becoming overconfident.\n",
    "            This keeps the training signal meaningful for the generator\n",
    "            and delays mode collapse.\n",
    "            '''\n",
    "            real_images += tf.random.normal(tf.shape(real_images), mean=0.0, stddev=0.05)  # Improvement\n",
    "            real_preds = self.discriminator(real_images, training=True)\n",
    "            fake_preds = self.discriminator(fake_images, training=True)\n",
    "\n",
    "            # Smoothed labels + noise\n",
    "            real_labels = tf.random.uniform(tf.shape(real_preds), minval=0.9, maxval=1.0)\n",
    "            fake_labels = tf.random.uniform(tf.shape(fake_preds), minval=0.0, maxval=0.1)\n",
    "\n",
    "            # === LABEL FLIPPING ===\n",
    "            if self.label_flipping_rate > 0.0:\n",
    "                real_flip_mask = tf.random.uniform(tf.shape(real_labels)) < self.label_flipping_rate\n",
    "                fake_flip_mask = tf.random.uniform(tf.shape(fake_labels)) < self.label_flipping_rate\n",
    "                real_labels = tf.where(real_flip_mask, tf.zeros_like(real_labels), real_labels)\n",
    "                fake_labels = tf.where(fake_flip_mask, tf.ones_like(fake_labels), fake_labels)\n",
    "\n",
    "            d_real_loss = self.loss_fn(real_labels, real_preds)\n",
    "            d_fake_loss = self.loss_fn(fake_labels, fake_preds)\n",
    "            d_loss = 0.5 * (d_real_loss + d_fake_loss)\n",
    "\n",
    "        d_grads = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # === GENERATOR TRAINING ===\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            fake_images = self.generator(random_latent, training=True)\n",
    "            fake_preds = self.discriminator(fake_images, training=False)\n",
    "            g_loss = self.loss_fn(tf.ones_like(fake_preds), fake_preds)\n",
    "\n",
    "        g_grads = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
    "\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d019bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c236b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_images=5, latent_dim=100, exp_name=\"default\"):\n",
    "        super().__init__()\n",
    "        self.num_images = num_images\n",
    "        self.latent_dim = latent_dim\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            z = tf.random.normal((self.num_images, self.latent_dim))\n",
    "            generated = self.model.generator(z, training=False)\n",
    "            imgs = (generated * 127.5 + 127.5) / 255.0\n",
    "\n",
    "            # Create subfolder for this experiment\n",
    "            save_dir = os.path.join(\"generated\", self.exp_name)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            image_path = os.path.join(save_dir, f\"epoch_{epoch + 1}.png\")\n",
    "\n",
    "            plt.figure(figsize=(15, 3))\n",
    "            for i in range(self.num_images):\n",
    "                plt.subplot(1, self.num_images, i + 1)\n",
    "                plt.imshow(imgs[i, :, :, 0], cmap='gray')\n",
    "                plt.axis('off')\n",
    "\n",
    "            plt.savefig(image_path)\n",
    "            plt.close()\n",
    "            print(f\"Wygenerowano próbki zapisane do: {image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc1ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "def run_experiment(loss_fn, d_lr, g_lr, g_decay=None, d_decay=None, exp_name=\"default\", epochs=50, label_flipping_rate=0.0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    print(f\"\\nRozpoczynam eksperyment: {exp_name}\")\n",
    "    if d_decay:\n",
    "        print(f\"Używam ExponentialDecay dla generatora: initial_lr={d_decay[0]}, decay_steps={d_decay[1]}, decay_rate={d_decay[2]}\")\n",
    "    \n",
    "    if g_decay:\n",
    "        print(f\"Używam ExponentialDecay dla dyskryminatora: initial_lr={g_decay[0]}, decay_steps={g_decay[1]}, decay_rate={g_decay[2]}\")\n",
    "\n",
    "    if not (g_decay or g_decay):\n",
    "        print(f\"Stałe learning rate - D: {d_lr}, G: {g_lr}\")\n",
    "    print(f\"Funkcja straty: {type(loss_fn).__name__}\")\n",
    "    print(f\"Odwracanie etykiet: {label_flipping_rate * 100:.1f}%\")\n",
    "\n",
    "    disc = build_discriminator()\n",
    "    gen = build_generator()\n",
    "    dcgan = DCGAN(disc, gen, latent_dim=100, label_flipping_rate=label_flipping_rate)\n",
    "\n",
    "    if d_decay:\n",
    "        d_lr = tf.keras.optimizers.schedules.ExponentialDecay(*d_decay)\n",
    "\n",
    "    if g_decay:\n",
    "        g_lr = tf.keras.optimizers.schedules.ExponentialDecay(*g_decay)\n",
    "\n",
    "    dcgan.compile(\n",
    "        d_optimizer=optimizers.Adam(learning_rate=d_lr, beta_1=0.4),\n",
    "        g_optimizer=optimizers.Adam(learning_rate=g_lr, beta_1=0.4)\n",
    "    )\n",
    "    dcgan.loss_fn = loss_fn\n",
    "\n",
    "    callbacks = [GANMonitor(num_images=5, latent_dim=100, exp_name=exp_name)]\n",
    "\n",
    "    history = dcgan.fit(\n",
    "        train_data,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    trained_epochs = len(history.history['g_loss'])\n",
    "    print(f\"✅ Zakończono trening: {trained_epochs} epok (max={epochs})\")\n",
    "\n",
    "    d_losses = history.history['d_loss']\n",
    "    g_losses = history.history['g_loss']\n",
    "\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    plot_dir = os.path.join(\"plots\", exp_name)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    plot_path = os.path.join(plot_dir, \"loss_plot.png\")\n",
    "    plt.figure()\n",
    "    plt.plot(d_losses, label='Discriminator Loss')\n",
    "    plt.plot(g_losses, label='Generator Loss')\n",
    "    plt.title(f\"Loss - {exp_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Wykres strat zapisano do: {plot_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7cd43",
   "metadata": {},
   "source": [
    "# Badania mnist z dobranymi parametrami\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8958506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rozpoczynam eksperyment: improved_75_d_5e-5_g_2e-4\n",
      "Stałe learning rate - D: 5e-05, G: 0.0002\n",
      "Funkcja straty: BinaryCrossentropy\n",
      "Odwracanie etykiet: 0.0%\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1747394125.705428    8686 meta_optimizer.cc:967] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'StatefulPartitionedCall/gradient_tape/Discriminator_5/leaky_re_lu_1_2/LeakyRelu/LeakyReluGrad' exist for missing node 'StatefulPartitionedCall/Discriminator_5/conv2d_1_2/BiasAdd'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 56ms/step - d_loss: 0.6220 - g_loss: 1.5737\n",
      "Epoch 2/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 56ms/step - d_loss: 0.6513 - g_loss: 0.8129\n",
      "Epoch 3/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 57ms/step - d_loss: 0.6181 - g_loss: 0.9230\n",
      "Epoch 4/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 58ms/step - d_loss: 0.6292 - g_loss: 1.3670\n",
      "Epoch 5/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - d_loss: 0.6086 - g_loss: 1.0923Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_5.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 57ms/step - d_loss: 0.6086 - g_loss: 1.0922\n",
      "Epoch 6/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 58ms/step - d_loss: 0.6070 - g_loss: 1.0395\n",
      "Epoch 7/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 58ms/step - d_loss: 0.7839 - g_loss: 1.1906\n",
      "Epoch 8/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.7844 - g_loss: 2.0289\n",
      "Epoch 9/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 56ms/step - d_loss: 0.5853 - g_loss: 1.6508\n",
      "Epoch 10/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - d_loss: 0.6656 - g_loss: 1.2235Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_10.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 57ms/step - d_loss: 0.6655 - g_loss: 1.2233\n",
      "Epoch 11/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 58ms/step - d_loss: 0.6219 - g_loss: 0.8849\n",
      "Epoch 12/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 58ms/step - d_loss: 0.6182 - g_loss: 1.0164\n",
      "Epoch 13/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 58ms/step - d_loss: 0.9800 - g_loss: 1.6103\n",
      "Epoch 14/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 58ms/step - d_loss: 0.6327 - g_loss: 0.9310\n",
      "Epoch 15/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - d_loss: 0.6254 - g_loss: 0.9733Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_15.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 58ms/step - d_loss: 0.6254 - g_loss: 0.9733\n",
      "Epoch 16/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 58ms/step - d_loss: 0.6125 - g_loss: 1.0366\n",
      "Epoch 17/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6143 - g_loss: 1.1223\n",
      "Epoch 18/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6323 - g_loss: 1.2015\n",
      "Epoch 19/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6051 - g_loss: 1.2082\n",
      "Epoch 20/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - d_loss: 0.7160 - g_loss: 1.7688Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_20.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.7160 - g_loss: 1.7686\n",
      "Epoch 21/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6202 - g_loss: 1.2092\n",
      "Epoch 22/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6474 - g_loss: 1.2840\n",
      "Epoch 23/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.7237 - g_loss: 1.3047\n",
      "Epoch 24/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6109 - g_loss: 1.1553\n",
      "Epoch 25/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - d_loss: 0.6371 - g_loss: 1.0756Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_25.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6371 - g_loss: 1.0756\n",
      "Epoch 26/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6575 - g_loss: 1.2185\n",
      "Epoch 27/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 4.5461 - g_loss: 9.3741\n",
      "Epoch 28/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.4844 - g_loss: 1.9496\n",
      "Epoch 29/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1463s\u001b[0m 781ms/step - d_loss: 1.1119 - g_loss: 1.7856\n",
      "Epoch 30/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - d_loss: 1.1241 - g_loss: 1.1824Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_30.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 1.1240 - g_loss: 1.1823\n",
      "Epoch 31/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 57ms/step - d_loss: 0.6006 - g_loss: 1.1463\n",
      "Epoch 32/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.6101 - g_loss: 1.1486\n",
      "Epoch 33/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 57ms/step - d_loss: 0.5992 - g_loss: 1.1242\n",
      "Epoch 34/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 57ms/step - d_loss: 0.6134 - g_loss: 1.1413\n",
      "Epoch 35/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - d_loss: 0.6596 - g_loss: 1.4255Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_35.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 57ms/step - d_loss: 0.6596 - g_loss: 1.4254\n",
      "Epoch 36/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 56ms/step - d_loss: 1.6629 - g_loss: 1.1895\n",
      "Epoch 37/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5397 - g_loss: 1.5166\n",
      "Epoch 38/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5445 - g_loss: 1.8585\n",
      "Epoch 39/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.7155 - g_loss: 1.8701\n",
      "Epoch 40/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - d_loss: 0.5364 - g_loss: 1.4553Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_40.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5364 - g_loss: 1.4553\n",
      "Epoch 41/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.6980 - g_loss: 1.5935\n",
      "Epoch 42/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5758 - g_loss: 1.2603\n",
      "Epoch 43/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5922 - g_loss: 1.2698\n",
      "Epoch 44/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5814 - g_loss: 1.2661\n",
      "Epoch 45/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - d_loss: 0.6070 - g_loss: 1.3369Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_45.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.6070 - g_loss: 1.3369\n",
      "Epoch 46/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 55ms/step - d_loss: 0.5781 - g_loss: 1.2898\n",
      "Epoch 47/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 4.0609 - g_loss: 0.6882\n",
      "Epoch 48/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 55ms/step - d_loss: 7.9574 - g_loss: 0.0056\n",
      "Epoch 49/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 55ms/step - d_loss: 0.5356 - g_loss: 1.7462\n",
      "Epoch 50/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - d_loss: 0.5640 - g_loss: 1.5876Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_50.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5640 - g_loss: 1.5876\n",
      "Epoch 51/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5656 - g_loss: 1.4238\n",
      "Epoch 52/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 55ms/step - d_loss: 0.9347 - g_loss: 2.0104\n",
      "Epoch 53/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 55ms/step - d_loss: 0.6153 - g_loss: 1.2945\n",
      "Epoch 54/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 55ms/step - d_loss: 0.6624 - g_loss: 1.3316\n",
      "Epoch 55/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - d_loss: 0.6509 - g_loss: 1.5393Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_55.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.6509 - g_loss: 1.5394\n",
      "Epoch 56/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5785 - g_loss: 1.3752\n",
      "Epoch 57/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.7230 - g_loss: 1.3782\n",
      "Epoch 58/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 55ms/step - d_loss: 0.5911 - g_loss: 1.4521\n",
      "Epoch 59/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 55ms/step - d_loss: 1.0206 - g_loss: 2.6704\n",
      "Epoch 60/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - d_loss: 0.4970 - g_loss: 2.5485Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_60.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.4970 - g_loss: 2.5486\n",
      "Epoch 61/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.4890 - g_loss: 2.7670\n",
      "Epoch 62/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.6517 - g_loss: 2.6471\n",
      "Epoch 63/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.5677 - g_loss: 2.6014\n",
      "Epoch 64/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.5295 - g_loss: 1.7610\n",
      "Epoch 65/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - d_loss: 0.7287 - g_loss: 2.2506Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_65.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.7287 - g_loss: 2.2504\n",
      "Epoch 66/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.6146 - g_loss: 2.3065\n",
      "Epoch 67/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.5010 - g_loss: 2.1410\n",
      "Epoch 68/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.5694 - g_loss: 3.4116\n",
      "Epoch 69/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.5689 - g_loss: 2.3738\n",
      "Epoch 70/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - d_loss: 3.5420 - g_loss: 0.9252Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_70.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 54ms/step - d_loss: 3.5410 - g_loss: 0.9253\n",
      "Epoch 71/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 54ms/step - d_loss: 0.5836 - g_loss: 1.4863\n",
      "Epoch 72/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.5837 - g_loss: 1.7047\n",
      "Epoch 73/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 54ms/step - d_loss: 0.5840 - g_loss: 1.4038\n",
      "Epoch 74/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 0.5522 - g_loss: 1.4590\n",
      "Epoch 75/75\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - d_loss: 3.2459 - g_loss: 5.6178Wygenerowano próbki zapisane do: generated/improved_75_d_5e-5_g_2e-4/epoch_75.png\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 57ms/step - d_loss: 3.2459 - g_loss: 5.6185\n",
      "✅ Zakończono trening: 75 epok (max=75)\n",
      "Wykres strat zapisano do: plots/improved_75_d_5e-5_g_2e-4/loss_plot.png\n"
     ]
    }
   ],
   "source": [
    "# === 6. RUN EXPERIMENTS ===\n",
    "\n",
    "base_epochs = 75\n",
    "\n",
    "d_lr, g_lr = (5e-5, 2e-4)\n",
    "run_experiment(\n",
    "    loss_fn=losses.BinaryCrossentropy(),\n",
    "    d_lr=d_lr,\n",
    "    g_lr=g_lr,\n",
    "    exp_name=f\"improved_{base_epochs}_d_5e-5_g_2e-4\",\n",
    "    epochs=base_epochs,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
